<!DOCTYPE html>
<html>
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=320">
        <link rel="stylesheet" media="screen, print" href="../../styles/reset-min.css">
        <link rel="stylesheet" media="screen, print" href="../../styles/fonts-min.css">
        <link rel="stylesheet" media="screen and (min-device-width: 481px), print" href="../../styles/cg.css">
        <link rel="stylesheet" media="only screen and (max-device-width: 480px)" href="../../styles/iphone.css">
        <!--[if IE]><link rel="stylesheet" media="screen, projection" href="../../styles/cg.css"><![endif]-->
        <!--[if lte IE 8]><script src="../../scripts/create-elements.js" type="text/javascript"></script><![endif]-->
        <script src="../../scripts/highlight.pack.js" type="text/javascript"></script>
        <link rel="stylesheet" media="screen, print" href="../../styles/github.css">
        <script type="text/javascript">hljs.initHighlightingOnLoad('bash','python','html','css');</script>
        <title>HTML から文章抽出して解析する際の Tips - yono.github.com</title>
    </head>
    <body>
        <header>
          <h1><a href="http://yono.github.com">yono.github.com</a></h1>
        </header>

        <nav>
            <ul>
                <li><a href="index.html">Index</a></li>
            </ul>
        </nav>

        <article>

<!-- Auto generated start -->

<h1>HTML から文章抽出して解析する際の Tips</h1>

<h2>概要</h2>

<p>HTML から文章などを抽出して解析する際の Tips をまとめたいと思います。
今回は、HTML 中に出現する単語を数える場合を想定しています。</p>

<p>このような解析の際に問題になるのは、文字コードと言語かと思います。</p>

<p>そこで、Python で文字コードと言語を判定する方法をまとめてみます。</p>

<h2>文字コード判定</h2>

<h3>1. HTTP ヘッダーの charset を確認する</h3>

<p>まずは HTTP ヘッダーの情報を利用するのが確実かと思います。
ただし、返された charset に Python が対応してない場合があるので、
codecs.lookup() で確認しています。
対応していない場合、もしくは charset の記述が間違っている場合は
LookupError となります。</p>

<pre><code>import urllib
import codecs
url = 'http://example.com'
response = urllib.urlopen(url)
charset = response.headers.getparam('charset')
html = response.read()

if charset != '':
    try:
        codecs.lookup(charset)
        html = html.decode(charset, 'replace')
    except:
        pass
</code></pre>

<h3>2. head タグ内の meta charset を確認する</h3>

<p>HTML 中に書かれている charset は特にスペルミスなどで間違っている
ことが多いので、注意する必要があります。
また、x-sjis や x-euc は Python の文字コード判定に弾かれるので
それぞれ Shift_JIS と EUC-JP として扱うようにしています。</p>

<pre><code>import re
char_re = re.compile(r"(?is)content=[\"'].*?;\s*charset=(.*?)[\"']")
enc_dic = {"x-sjis": "Shift_JIS", "x-euc": "EUC-JP"}
result = char_re.search(html)
if result is not None:
    enc = resulg.group(1)
    if enc in enc_dic:
        enc = enc_dic[enc]
else:
    enc = ''
</code></pre>

<h3>3. Try and Error で手当たりしだいに decode する</h3>

<p>ここに関しては<a href="http://mimosa-pudica.net/python-tips.html">Python Tips</a>
を参考、というかほぼそのまま使わせていただいています。</p>

<p>判定する文字コードの順番を変えてますが、これは確かこっちの方が正解率が
高かったとかそういう理由だったと思います。</p>

<pre><code>encodings = [
        "ascii",
        "utf-8",
        "euc-jp",
        "cp932",
        "iso-2022-jp",
]


def detect( text ):
        bestScore = -1
        bestEnc = None
        for enc in encodings:
                try:
                        unicode( text, enc )
                except UnicodeDecodeError, err:
                        if err.end &gt; bestScore:
                                bestScore = err.end
                                bestEnc = enc
                else:
                        return {
                                "encoding": enc,
                                "confidence": 1.0,
                        }

        return {
                "encoding": bestEnc,
                "confidence": bestScore / (bestScore + 2.0),
</code></pre>

<h3>4. chardet を使う</h3>

<p>chardet(<a href="http://chardet.feedparser.org/">Universal Encode Detector</a>) とは Python 用の文字コード判定モジュールです。
精度は決して悪くないんですが、処理が重たいので優先順位を低めにして
使用しています。</p>

<pre><code>import chardet
enc = chardet.detect(html)['encoding']
</code></pre>

<h2>言語判定</h2>

<p>ngram.py と Lingua::LanguageGuesser の言語モデルを組み合わせる方法を
使ってました。</p>

<p>ngram.py: <a href="http://thomas.mangin.me.uk/">http://thomas.mangin.me.uk/</a></p>

<p>ページ右側の検索窓で ngram.py で検索するとダウンロードリンクを見つけることができます。</p>

<p>もしくは <a href="http://thomas.mangin.me.uk/data/source/ngram.py">ngram.py</a> から直接。</p>

<p>Lingua::LanguageGuesser: <a href="http://gensen.dl.itc.u-tokyo.ac.jp/LanguageGuesser/hajimete_monogatari.html">http://gensen.dl.itc.u-tokyo.ac.jp/LanguageGuesser/hajimete_monogatari.html</a></p>

<p>この二つのモジュールは TextCat という Perl で書かれた言語判定スクリプトの</p>

<ul>
<li>Python 移植版 → ngram.py</li>
<li>Perl モジュール化 &amp; UTF-8 への対応を強化 → Lingua::LanguageGuesser</li>
</ul>


<p>という関係性があります。</p>

<p>ngram.py を動かすには言語モデル（言語ごとのプロファイル）を記録したファイルが必要です。配布元では TextCat の言語モデルを使用するように、とされてますがこれに Lingua::LanguageGuesser の言語モデルを使用することで UTF-8 へ対応できるようにします。</p>

<p>ちなみに、UTF-8 かどうかで判定に使用する言語モデルを変更する必要が
あるので、使用する場合はその辺りも考慮する必要があります。</p>

<pre><code>imporg ngram

n = ngram._NGram()
if enc == 'UTF-8':
    l = ngram.NGram('/path/to/utf8/modeldir')
else:
    l = ngram.NGram('/path/to/nonutf8/modeldir')
lang = l.classify(text)
</code></pre>

<p>いろいろはしょってますがこんな感じで。ちなみに、LanguageGuesser の
言語モデルは /usr/lib/perl5/site_perl/5.8.8/Lingua/LM_utf8/
辺りにありました（CentOS の場合）。</p>

<p>一つ注意点として。この方法だと、例えば英語スパムが大量に付けられた
ブログ記事を英語と判定してしまったり、ソースコードが大量に書かれた
記事を英語として判定してしまったり、といったことが起こりえます。</p>

<p>そのような場合には、あらかじめ記事本文のみを抽出したり、
ソースコードを取り除くといった処理が必要になります。</p>


<!-- Auto generated end -->

        </article>

        <footer>
            <p>Copyright &#169; 2010 <a href="mailto:yono.05@gmail.com">yono</a> All rights reserved.</p>
            <p>Powerd by <a href="http://github.com/Tomohiro/cg" title="cg - A Ruby based ContentsGenerator">cg - A Ruby based contents generator</a></p>
        </footer>
    </body>
</html>
